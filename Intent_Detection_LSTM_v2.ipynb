{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intent_Detection_LSTM_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhay43/FaceApp-with-Deep-Learning/blob/master/Intent_Detection_LSTM_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Vxn7338rLp",
        "colab_type": "text"
      },
      "source": [
        "#Intent Classification using LSTM \n",
        "This use-case provides a demo of how LSTM can be used for Intent classification in texts. \n",
        "\n",
        "##Workflow:\n",
        "\n",
        "1.   Understanding the problem\n",
        "2.   Reading the data and understanding it\n",
        "3.   Data Preprocessing\n",
        "4.   Build LSTM model\n",
        "5.   Train & Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYTbGtop9jP1",
        "colab_type": "text"
      },
      "source": [
        "##1. Understanding the problem\n",
        "Intent Classification is the automated association of text to a specific intention. For example: Let's say you are writing an email to one of the Airlines and the text of the same is 'Can you please cancel my ticket with PNR 123456'. The intent of the customer here is 'Cancellation of Air Ticket'.\n",
        "\n",
        "The idea of this use case to introduce the concept of Intent classification and how can LSTM be used to solve this. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEN-kTBx_4sC",
        "colab_type": "text"
      },
      "source": [
        "###Import the necessary libraries\n",
        "Please load the following packages before you proceed further. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k38KZeu8uUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8ecaff43-f833-4b0f-ad27-b150234d9a98"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import nltk\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder as oneHot\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.stem import PorterStemmer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Input, Embedding, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy as cce\n",
        "from tensorflow.keras.activations import relu, softmax\n",
        "from tensorflow.keras.initializers import he_uniform, glorot_uniform\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdGOHrlVjeDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myB1ICGyjih2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c54cce71-5629-4af2-e473-17092c936e91"
      },
      "source": [
        "tensorflow.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjqYhVvAY8i",
        "colab_type": "text"
      },
      "source": [
        "##2. Collecting the Data\n",
        "The ATIS(Air Travel Information System) data is a rich corpus that contains natural language text used by general public to book flight tickets, enquire about flight timings, prices etc. \n",
        "\n",
        "The Train and test data can be downloaded from https://www.kaggle.com/hassanamin/atis-airlinetravelinformationsystem\n",
        "\n",
        "There are 2 columns in each of the above datasets. First column is 'target' which is the output we will be classifying and second column is 'text' which is the user input asking for queries related to flights. \n",
        "\n",
        "Basically 'target' is the intent of the customer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXCF5zWiwcYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read the train and test datasets with column names as target and text\n",
        "train= pd.read_csv('/content/datasets_284285_585165_atis_intents_train.csv',\n",
        "                       names= [\"target\", \"text\"])\n",
        "\n",
        "test= pd.read_csv('/content/datasets_284285_585165_atis_intents_test.csv',\n",
        "                       names= [\"target\", \"text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcA1sbFmrc2c",
        "colab_type": "text"
      },
      "source": [
        "####This is how the data looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcBN6Hn2rhjD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "2c6bf42c-228b-4728-85c5-4b7d13ea9556"
      },
      "source": [
        "train.head(10) #Get Top 10 rows from train dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>atis_flight</td>\n",
              "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>atis_flight</td>\n",
              "      <td>what flights are available from pittsburgh to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>atis_flight_time</td>\n",
              "      <td>what is the arrival time in san francisco for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>atis_airfare</td>\n",
              "      <td>cheapest airfare from tacoma to orlando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>atis_airfare</td>\n",
              "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>atis_flight</td>\n",
              "      <td>i need a flight tomorrow from columbus to min...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>atis_aircraft</td>\n",
              "      <td>what kind of aircraft is used on a flight fro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>atis_flight</td>\n",
              "      <td>show me the flights from pittsburgh to los an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>atis_flight</td>\n",
              "      <td>all flights from boston to washington</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>atis_ground_service</td>\n",
              "      <td>what kind of ground transportation is availab...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                target                                               text\n",
              "0          atis_flight   i want to fly from boston at 838 am and arriv...\n",
              "1          atis_flight   what flights are available from pittsburgh to...\n",
              "2     atis_flight_time   what is the arrival time in san francisco for...\n",
              "3         atis_airfare            cheapest airfare from tacoma to orlando\n",
              "4         atis_airfare   round trip fares from pittsburgh to philadelp...\n",
              "5          atis_flight   i need a flight tomorrow from columbus to min...\n",
              "6        atis_aircraft   what kind of aircraft is used on a flight fro...\n",
              "7          atis_flight   show me the flights from pittsburgh to los an...\n",
              "8          atis_flight              all flights from boston to washington\n",
              "9  atis_ground_service   what kind of ground transportation is availab..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3uwDKh5k0pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e86e6870-bbe3-4267-d5ff-8a46ed09537d"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4834, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQFpMc7pr5My",
        "colab_type": "text"
      },
      "source": [
        " Check the number of intents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63HCDQh2r4Ue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "bcf11920-70eb-44a9-8e5f-f0fd32741551"
      },
      "source": [
        "train['target'].value_counts() #Get counts of different types of target variable in train data. We will not be using this anywhere but it is just for the overview of the data.  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "atis_flight            3666\n",
              "atis_airfare            423\n",
              "atis_ground_service     255\n",
              "atis_airline            157\n",
              "atis_abbreviation       147\n",
              "atis_aircraft            81\n",
              "atis_flight_time         54\n",
              "atis_quantity            51\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLnuCxA6lIbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[00001000] [10000000] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIPGsV-2l-Pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1b249af-c19b-4485-c8ec-d1051bb2a9ed"
      },
      "source": [
        "train.target.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4834,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vaan11BFwPUM",
        "colab_type": "text"
      },
      "source": [
        "### 3. Preprocessing the Data\n",
        "We will be doing the following preprocessing steps to get the desired format of the data. \n",
        "\n",
        "1. Perform One Hot Encoding on the target variable of both train & test datasets.\n",
        "2. Convert the text into lower case.\n",
        "3. Tokenize the words.\n",
        "4. Remove stop words.\n",
        "5. Perform stemming & normalization.\n",
        "6. Convert texts into sequences.\n",
        "7. Pad the sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASGgSyGwzgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encode_target= oneHot().fit(np.array(train.target).reshape(-1,1)) #We perform one hot encoding on the target variable to convert into a matrix of 0s and 1s. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgrmhYVumCLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "94b5e301-0d52-4c00-91e7-d8f825ed2715"
      },
      "source": [
        "encode_target.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['x0_atis_abbreviation', 'x0_atis_aircraft', 'x0_atis_airfare',\n",
              "       'x0_atis_airline', 'x0_atis_flight', 'x0_atis_flight_time',\n",
              "       'x0_atis_ground_service', 'x0_atis_quantity'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY1ZPSK1CjEP",
        "colab_type": "text"
      },
      "source": [
        "Perform One Hot Encoding on the target variable. The output of this step would be an array with 0s and 1s. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fN8Y4bOzXk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_target_encoded= encode_target.transform(np.array(train.target).reshape(-1,1)).toarray()\n",
        "test_target_encoded= encode_target.transform(np.array(test.target).reshape(-1,1)).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdQU0XJzmxb5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "cbb9821e-946a-4590-f9a2-6915485900f4"
      },
      "source": [
        "train_target_encoded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKFef7TEm-TN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2fdba18-c658-4d60-f905-c72de5a3a2b3"
      },
      "source": [
        "test_target_encoded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2HirMzwDf5q",
        "colab_type": "text"
      },
      "source": [
        "Convert text to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiUYFX5Ywcnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"text\"]= train.text.map(lambda l: l.lower())\n",
        "test[\"text\"]= test.text.map(lambda l: l.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-21gL9dynJ3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "2b85a1d9-f436-45b7-b90e-8a64f57c9b28"
      },
      "source": [
        "train.text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        i want to fly from boston at 838 am and arriv...\n",
              "1        what flights are available from pittsburgh to...\n",
              "2        what is the arrival time in san francisco for...\n",
              "3                 cheapest airfare from tacoma to orlando\n",
              "4        round trip fares from pittsburgh to philadelp...\n",
              "                              ...                        \n",
              "4829     what is the airfare for flights from denver t...\n",
              "4830     do you have any flights from denver to baltim...\n",
              "4831            which airlines fly into and out of denver\n",
              "4832     does continental fly from boston to san franc...\n",
              "4833     is there a delta flight from denver to san fr...\n",
              "Name: text, Length: 4834, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qinvp54EDer0",
        "colab_type": "text"
      },
      "source": [
        "Next step is to tokenize the text. We use word_tokenize function from nltk library for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJR50ShUwcht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"text\"]= train.text.map(word_tokenize)\n",
        "test[\"text\"]= test.text.map(word_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS2kgyqqEN1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "3ab01652-6a54-4335-f302-23147968bf29"
      },
      "source": [
        "#Output of the above exercise looks like this\n",
        "train[\"text\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [i, want, to, fly, from, boston, at, 838, am, ...\n",
              "1       [what, flights, are, available, from, pittsbur...\n",
              "2       [what, is, the, arrival, time, in, san, franci...\n",
              "3          [cheapest, airfare, from, tacoma, to, orlando]\n",
              "4       [round, trip, fares, from, pittsburgh, to, phi...\n",
              "                              ...                        \n",
              "4829    [what, is, the, airfare, for, flights, from, d...\n",
              "4830    [do, you, have, any, flights, from, denver, to...\n",
              "4831    [which, airlines, fly, into, and, out, of, den...\n",
              "4832    [does, continental, fly, from, boston, to, san...\n",
              "4833    [is, there, a, delta, flight, from, denver, to...\n",
              "Name: text, Length: 4834, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqBeYgcwEez3",
        "colab_type": "text"
      },
      "source": [
        "Eliminate stop words. 'english' dictionary from nltk.corpus library is used for this purpose. We also remove punctuation along with the removal of stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlsQpXeOwcfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_data_rm_stop(strings, stop_list):\n",
        "    sw= [str for str in strings if str not in stop_list]\n",
        "    return sw\n",
        "\n",
        "stop_words= stopwords.words(\"english\")\n",
        "rm_punc_stop= list(set(punctuation))+ stop_words  #Remove punctuation and stop words\n",
        "\n",
        "train[\"text\"]= train.text.map(lambda dataframe: clean_data_rm_stop(dataframe, rm_punc_stop))\n",
        "test[\"text\"]= test.text.map(lambda dataframe: clean_data_rm_stop(dataframe, rm_punc_stop))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prLIz1P0n2h5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "f4a17be3-bb68-4f5a-d5fa-2af3737b2618"
      },
      "source": [
        "train.text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [want, fly, boston, 838, arrive, denver, 1110,...\n",
              "1       [flights, available, pittsburgh, baltimore, th...\n",
              "2       [arrival, time, san, francisco, 755, flight, l...\n",
              "3                    [cheapest, airfare, tacoma, orlando]\n",
              "4       [round, trip, fares, pittsburgh, philadelphia,...\n",
              "                              ...                        \n",
              "4829    [airfare, flights, denver, pittsburgh, delta, ...\n",
              "4830            [flights, denver, baltimore, via, dallas]\n",
              "4831                              [airlines, fly, denver]\n",
              "4832    [continental, fly, boston, san, francisco, sto...\n",
              "4833              [delta, flight, denver, san, francisco]\n",
              "Name: text, Length: 4834, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUfp5HlrEd_z",
        "colab_type": "text"
      },
      "source": [
        "Stemming & Normalizing\n",
        "\n",
        "\n",
        "*   Stemming helps in reducing the word to the root form.\n",
        "*   Normalizing is the process of transforming text into a standard form. Eg: Gud will be converted to good etc.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTyFrktRwcch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    return \" \".join(text)\n",
        "\n",
        "#We use PorterStemmer function from nltk.stem library.\n",
        "stem_func= PorterStemmer()\n",
        "\n",
        "train[\"text\"]= train.text.map(lambda s: [stem_func.stem(x) for x in s])\n",
        "train[\"text\"]= train.text.apply(normalize)\n",
        "\n",
        "test[\"text\"]= test.text.map(lambda s: [stem_func.stem(x) for x in s])\n",
        "test[\"text\"]= test.text.apply(normalize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBYGiCz2oFNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "5005767f-2654-4bb5-d0ea-37e38a584eec"
      },
      "source": [
        "train.text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0              want fli boston 838 arriv denver 1110 morn\n",
              "1          flight avail pittsburgh baltimor thursday morn\n",
              "2       arriv time san francisco 755 flight leav washi...\n",
              "3                          cheapest airfar tacoma orlando\n",
              "4       round trip fare pittsburgh philadelphia 1000 d...\n",
              "                              ...                        \n",
              "4829         airfar flight denver pittsburgh delta airlin\n",
              "4830                     flight denver baltimor via dalla\n",
              "4831                                    airlin fli denver\n",
              "4832       continent fli boston san francisco stop denver\n",
              "4833                    delta flight denver san francisco\n",
              "Name: text, Length: 4834, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wp1q6YAGneR",
        "colab_type": "text"
      },
      "source": [
        "Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjiTh4B-yyKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use Tokenizer from tensorflow.keras.preprocessing.text library\n",
        "num_words=10000\n",
        "text_tokenizer= Tokenizer(num_words)\n",
        "text_tokenizer.fit_on_texts(train.text) #fit_on_texts - creates the vocabulary index based on word frequency.\n",
        "\n",
        "tokenized_train_data= text_tokenizer.texts_to_sequences(train.text) #Converting texts to sequences\n",
        "tokenized_test_data= text_tokenizer.texts_to_sequences(test.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyeGl8vKokSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_len = len(text_tokenizer.index_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dGR7C4_2MT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50a5ce01-b57e-4b8b-d3a4-d819789cd428"
      },
      "source": [
        "text_tokenizer.index_word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'flight',\n",
              " 2: 'boston',\n",
              " 3: 'show',\n",
              " 4: 'san',\n",
              " 5: 'denver',\n",
              " 6: 'francisco',\n",
              " 7: 'atlanta',\n",
              " 8: 'pittsburgh',\n",
              " 9: 'dalla',\n",
              " 10: 'baltimor',\n",
              " 11: 'philadelphia',\n",
              " 12: 'leav',\n",
              " 13: 'airlin',\n",
              " 14: 'like',\n",
              " 15: 'list',\n",
              " 16: 'fare',\n",
              " 17: 'arriv',\n",
              " 18: 'washington',\n",
              " 19: 'fli',\n",
              " 20: 'pleas',\n",
              " 21: 'morn',\n",
              " 22: 'pm',\n",
              " 23: 'would',\n",
              " 24: 'first',\n",
              " 25: 'wednesday',\n",
              " 26: 'oakland',\n",
              " 27: 'ground',\n",
              " 28: \"'d\",\n",
              " 29: 'transport',\n",
              " 30: 'trip',\n",
              " 31: 'class',\n",
              " 32: 'cheapest',\n",
              " 33: 'need',\n",
              " 34: 'citi',\n",
              " 35: 'go',\n",
              " 36: 'round',\n",
              " 37: 'avail',\n",
              " 38: 'afternoon',\n",
              " 39: 'american',\n",
              " 40: 'one',\n",
              " 41: 'give',\n",
              " 42: 'want',\n",
              " 43: 'way',\n",
              " 44: 'new',\n",
              " 45: 'thursday',\n",
              " 46: 'york',\n",
              " 47: 'earliest',\n",
              " 48: 'nonstop',\n",
              " 49: 'monday',\n",
              " 50: 'dc',\n",
              " 51: 'stop',\n",
              " 52: 'tuesday',\n",
              " 53: 'unit',\n",
              " 54: 'la',\n",
              " 55: 'inform',\n",
              " 56: 'st',\n",
              " 57: 'milwauke',\n",
              " 58: 'find',\n",
              " 59: 'airport',\n",
              " 60: 'sunday',\n",
              " 61: 'twenti',\n",
              " 62: 'miami',\n",
              " 63: 'even',\n",
              " 64: 'vega',\n",
              " 65: 'delta',\n",
              " 66: 'noon',\n",
              " 67: 'newark',\n",
              " 68: 'chicago',\n",
              " 69: \"o'clock\",\n",
              " 70: 'charlott',\n",
              " 71: 'saturday',\n",
              " 72: 'phoenix',\n",
              " 73: 'august',\n",
              " 74: \"'s\",\n",
              " 75: 'continent',\n",
              " 76: 'friday',\n",
              " 77: 'juli',\n",
              " 78: 'diego',\n",
              " 79: 'next',\n",
              " 80: 'earli',\n",
              " 81: 'fort',\n",
              " 82: 'worth',\n",
              " 83: 'toronto',\n",
              " 84: 'depart',\n",
              " 85: 'us',\n",
              " 86: 'seattl',\n",
              " 87: 'houston',\n",
              " 88: 'seventh',\n",
              " 89: 'serv',\n",
              " 90: 'air',\n",
              " 91: 'orlando',\n",
              " 92: 'tell',\n",
              " 93: '5',\n",
              " 94: 'code',\n",
              " 95: 'kansa',\n",
              " 96: 'indianapoli',\n",
              " 97: 'latest',\n",
              " 98: 'tomorrow',\n",
              " 99: 'aircraft',\n",
              " 100: 'cleveland',\n",
              " 101: 'stopov',\n",
              " 102: 'time',\n",
              " 103: 'lo',\n",
              " 104: 'angel',\n",
              " 105: 'salt',\n",
              " 106: 'lake',\n",
              " 107: 'cost',\n",
              " 108: 'use',\n",
              " 109: 'ticket',\n",
              " 110: 'downtown',\n",
              " 111: 'around',\n",
              " 112: 'type',\n",
              " 113: '6',\n",
              " 114: '10',\n",
              " 115: 'montreal',\n",
              " 116: 'memphi',\n",
              " 117: 'mean',\n",
              " 118: 'see',\n",
              " 119: 'twa',\n",
              " 120: 'travel',\n",
              " 121: 'june',\n",
              " 122: '8',\n",
              " 123: 'may',\n",
              " 124: 'could',\n",
              " 125: 'mani',\n",
              " 126: 'tampa',\n",
              " 127: 'nashvil',\n",
              " 128: 'petersburg',\n",
              " 129: 'expens',\n",
              " 130: 'jose',\n",
              " 131: \"'m\",\n",
              " 132: '7',\n",
              " 133: 'get',\n",
              " 134: 'dollar',\n",
              " 135: 'return',\n",
              " 136: 'book',\n",
              " 137: '12',\n",
              " 138: 'minneapoli',\n",
              " 139: 'loui',\n",
              " 140: 'know',\n",
              " 141: 'meal',\n",
              " 142: 'tacoma',\n",
              " 143: 'cincinnati',\n",
              " 144: 'least',\n",
              " 145: 'make',\n",
              " 146: 'daili',\n",
              " 147: 'servic',\n",
              " 148: '4',\n",
              " 149: 'okay',\n",
              " 150: 'much',\n",
              " 151: 'day',\n",
              " 152: 'price',\n",
              " 153: 'long',\n",
              " 154: 'beach',\n",
              " 155: 'economi',\n",
              " 156: 'coach',\n",
              " 157: 'lowest',\n",
              " 158: 'novemb',\n",
              " 159: 'less',\n",
              " 160: 'intern',\n",
              " 161: 'last',\n",
              " 162: 'paul',\n",
              " 163: 'detroit',\n",
              " 164: 'kind',\n",
              " 165: 'night',\n",
              " 166: 'septemb',\n",
              " 167: 'california',\n",
              " 168: 'love',\n",
              " 169: 'field',\n",
              " 170: 'second',\n",
              " 171: 'northwest',\n",
              " 172: 'connect',\n",
              " 173: '9',\n",
              " 174: 'columbu',\n",
              " 175: 'decemb',\n",
              " 176: 'third',\n",
              " 177: 'gener',\n",
              " 178: 'mitchel',\n",
              " 179: 'schedul',\n",
              " 180: '1000',\n",
              " 181: '2',\n",
              " 182: 'direct',\n",
              " 183: 'look',\n",
              " 184: 'breakfast',\n",
              " 185: 'burbank',\n",
              " 186: 'possibl',\n",
              " 187: 'arrang',\n",
              " 188: 'april',\n",
              " 189: 'take',\n",
              " 190: 'plane',\n",
              " 191: 'fifth',\n",
              " 192: 'fourth',\n",
              " 193: 'eastern',\n",
              " 194: 'dl',\n",
              " 195: 'late',\n",
              " 196: 'eighth',\n",
              " 197: 'goe',\n",
              " 198: '1991',\n",
              " 199: 'interest',\n",
              " 200: 'busi',\n",
              " 201: 'dinner',\n",
              " 202: 'stand',\n",
              " 203: 'twentieth',\n",
              " 204: 'week',\n",
              " 205: 'also',\n",
              " 206: 'two',\n",
              " 207: 'car',\n",
              " 208: 'ontario',\n",
              " 209: 'restrict',\n",
              " 210: '3',\n",
              " 211: 'ua',\n",
              " 212: 'airfar',\n",
              " 213: 'fifteenth',\n",
              " 214: 'westchest',\n",
              " 215: 'via',\n",
              " 216: 'today',\n",
              " 217: 'ninth',\n",
              " 218: 'back',\n",
              " 219: 'sixth',\n",
              " 220: 'display',\n",
              " 221: 'counti',\n",
              " 222: 'ap',\n",
              " 223: 'land',\n",
              " 224: 'thirtieth',\n",
              " 225: 'bwi',\n",
              " 226: 'midwest',\n",
              " 227: 'express',\n",
              " 228: 'rental',\n",
              " 229: '1',\n",
              " 230: 'departur',\n",
              " 231: 'weekday',\n",
              " 232: 'florida',\n",
              " 233: 'eleventh',\n",
              " 234: 'tenth',\n",
              " 235: 'qx',\n",
              " 236: 'either',\n",
              " 237: 'octob',\n",
              " 238: 'north',\n",
              " 239: 'carolina',\n",
              " 240: 'number',\n",
              " 241: 'smallest',\n",
              " 242: 'offer',\n",
              " 243: 'f',\n",
              " 244: 'lunch',\n",
              " 245: 'twelfth',\n",
              " 246: 'live',\n",
              " 247: 'h',\n",
              " 248: 'jersey',\n",
              " 249: 'colorado',\n",
              " 250: '57',\n",
              " 251: 'explain',\n",
              " 252: 'shortest',\n",
              " 253: 'thank',\n",
              " 254: 'sixteenth',\n",
              " 255: 'fourteenth',\n",
              " 256: 'thirti',\n",
              " 257: 'abbrevi',\n",
              " 258: 'provid',\n",
              " 259: 'airplan',\n",
              " 260: 'hi',\n",
              " 261: 'januari',\n",
              " 262: 'ewr',\n",
              " 263: 'march',\n",
              " 264: 'nineteenth',\n",
              " 265: 'guardia',\n",
              " 266: 'seventeenth',\n",
              " 267: 'layov',\n",
              " 268: \"'re\",\n",
              " 269: 'dfw',\n",
              " 270: 'takeoff',\n",
              " 271: 'right',\n",
              " 272: 'come',\n",
              " 273: 'canadian',\n",
              " 274: 'ye',\n",
              " 275: 'yn',\n",
              " 276: '466',\n",
              " 277: '11',\n",
              " 278: 'limousin',\n",
              " 279: 'hp',\n",
              " 280: 'hour',\n",
              " 281: 'q',\n",
              " 282: 'qw',\n",
              " 283: 'seat',\n",
              " 284: 'logan',\n",
              " 285: 'mco',\n",
              " 286: 'sfo',\n",
              " 287: '630',\n",
              " 288: 'tri',\n",
              " 289: 'start',\n",
              " 290: 'canada',\n",
              " 291: 'let',\n",
              " 292: '281',\n",
              " 293: 'later',\n",
              " 294: 'qo',\n",
              " 295: 'februari',\n",
              " 296: '838',\n",
              " 297: 'reserv',\n",
              " 298: 'midnight',\n",
              " 299: 'southwest',\n",
              " 300: 'thrift',\n",
              " 301: '530',\n",
              " 302: 'differ',\n",
              " 303: 'boe',\n",
              " 304: '430',\n",
              " 305: 'jfk',\n",
              " 306: 'eighteenth',\n",
              " 307: 'origin',\n",
              " 308: 'includ',\n",
              " 309: 'lufthansa',\n",
              " 310: 'ff',\n",
              " 311: 'ea',\n",
              " 312: '747',\n",
              " 313: 'sometim',\n",
              " 314: \"'ll\",\n",
              " 315: 'anywher',\n",
              " 316: 'area',\n",
              " 317: '230',\n",
              " 318: 'cheap',\n",
              " 319: 'capac',\n",
              " 320: 'ohio',\n",
              " 321: 'pennsylvania',\n",
              " 322: 'sixteen',\n",
              " 323: 'hello',\n",
              " 324: '1115',\n",
              " 325: '1245',\n",
              " 326: '1992',\n",
              " 327: '825',\n",
              " 328: 'help',\n",
              " 329: 'plan',\n",
              " 330: '718',\n",
              " 331: 'follow',\n",
              " 332: 'amount',\n",
              " 333: '2100',\n",
              " 334: 'rent',\n",
              " 335: 'peopl',\n",
              " 336: 'destin',\n",
              " 337: 'anoth',\n",
              " 338: 'georgia',\n",
              " 339: 'request',\n",
              " 340: 'america',\n",
              " 341: 'west',\n",
              " 342: 'rate',\n",
              " 343: 'sorri',\n",
              " 344: 'head',\n",
              " 345: 'wish',\n",
              " 346: 'close',\n",
              " 347: '813',\n",
              " 348: 'nationair',\n",
              " 349: 'maximum',\n",
              " 350: 'choic',\n",
              " 351: '80',\n",
              " 352: 'sa',\n",
              " 353: 'fn',\n",
              " 354: 'co',\n",
              " 355: '555',\n",
              " 356: 'defin',\n",
              " 357: '1291',\n",
              " 358: '21',\n",
              " 359: 'total',\n",
              " 360: 'texa',\n",
              " 361: 'nw',\n",
              " 362: 'noontim',\n",
              " 363: 'ten',\n",
              " 364: 'near',\n",
              " 365: 'tennesse',\n",
              " 366: 'dc10',\n",
              " 367: '296',\n",
              " 368: '934',\n",
              " 369: 'reach',\n",
              " 370: 'well',\n",
              " 371: '270',\n",
              " 372: '415',\n",
              " 373: 'six',\n",
              " 374: 'great',\n",
              " 375: 'three',\n",
              " 376: '1110',\n",
              " 377: '755',\n",
              " 378: '720',\n",
              " 379: 'c',\n",
              " 380: 'snack',\n",
              " 381: 'begin',\n",
              " 382: 'sure',\n",
              " 383: \"n't\",\n",
              " 384: 'approxim',\n",
              " 385: 'say',\n",
              " 386: 'prefer',\n",
              " 387: 'four',\n",
              " 388: '737',\n",
              " 389: 'minnesota',\n",
              " 390: 'highest',\n",
              " 391: '1700',\n",
              " 392: 'ord',\n",
              " 393: '2134',\n",
              " 394: 'tower',\n",
              " 395: '1765',\n",
              " 396: 'soon',\n",
              " 397: 'eight',\n",
              " 398: 'quebec',\n",
              " 399: 'repeat',\n",
              " 400: 'm80',\n",
              " 401: 'cp',\n",
              " 402: 'carri',\n",
              " 403: 'passeng',\n",
              " 404: 'taxi',\n",
              " 405: '201',\n",
              " 406: 'philli',\n",
              " 407: 'dinnertim',\n",
              " 408: 'lastest',\n",
              " 409: '1222',\n",
              " 410: 'limo',\n",
              " 411: '3724',\n",
              " 412: 'stapleton',\n",
              " 413: '343',\n",
              " 414: 'option',\n",
              " 415: '1145',\n",
              " 416: '932',\n",
              " 417: 'without',\n",
              " 418: 'b',\n",
              " 419: 'midway',\n",
              " 420: '217',\n",
              " 421: 'bound',\n",
              " 422: '727',\n",
              " 423: '324',\n",
              " 424: 'alaska',\n",
              " 425: 'michigan',\n",
              " 426: 'train',\n",
              " 427: 'arizona',\n",
              " 428: 'along',\n",
              " 429: 'friend',\n",
              " 430: 'thirteenth',\n",
              " 431: 'transcontinent',\n",
              " 432: 'missouri',\n",
              " 433: 'utah',\n",
              " 434: 'dull',\n",
              " 435: '269',\n",
              " 436: 'ac',\n",
              " 437: 'turboprop',\n",
              " 438: '757',\n",
              " 439: 'atl',\n",
              " 440: 'name',\n",
              " 441: 'month',\n",
              " 442: '845',\n",
              " 443: 'ap57',\n",
              " 444: 'run',\n",
              " 445: 'buy',\n",
              " 446: 'ap80',\n",
              " 447: 'still',\n",
              " 448: 'itinerari',\n",
              " 449: 'describ',\n",
              " 450: 'actual',\n",
              " 451: '505',\n",
              " 452: 'proper',\n",
              " 453: '329',\n",
              " 454: '352',\n",
              " 455: '1024',\n",
              " 456: '100',\n",
              " 457: '852',\n",
              " 458: '615',\n",
              " 459: 'mealtim',\n",
              " 460: '1288',\n",
              " 461: '257',\n",
              " 462: 'across',\n",
              " 463: 'contin',\n",
              " 464: 'f28',\n",
              " 465: 'overnight',\n",
              " 466: 'local',\n",
              " 467: 'rout',\n",
              " 468: '746',\n",
              " 469: 'repres',\n",
              " 470: 'databas',\n",
              " 471: 'tran',\n",
              " 472: 'world',\n",
              " 473: '1030',\n",
              " 474: '1130',\n",
              " 475: 'discount',\n",
              " 476: '2153',\n",
              " 477: 'thereaft',\n",
              " 478: '71',\n",
              " 479: 'supper',\n",
              " 480: 'bna',\n",
              " 481: '106',\n",
              " 482: 'afterward',\n",
              " 483: '345',\n",
              " 484: '19',\n",
              " 485: '82',\n",
              " 486: '139',\n",
              " 487: '1100',\n",
              " 488: '420',\n",
              " 489: 'seven',\n",
              " 490: 'kindli',\n",
              " 491: 'place',\n",
              " 492: '1026',\n",
              " 493: '124',\n",
              " 494: 'fifteen',\n",
              " 495: 'oh',\n",
              " 496: 'year',\n",
              " 497: \"o'har\",\n",
              " 498: '815',\n",
              " 499: '928',\n",
              " 500: 'bur',\n",
              " 501: '315',\n",
              " 502: '1039',\n",
              " 503: 'longest',\n",
              " 504: '200',\n",
              " 505: 'must',\n",
              " 506: '297',\n",
              " 507: 'question',\n",
              " 508: 'lay',\n",
              " 509: '650',\n",
              " 510: 'tonight',\n",
              " 511: 'ls',\n",
              " 512: '210',\n",
              " 513: '1600',\n",
              " 514: 'k',\n",
              " 515: 'aa',\n",
              " 516: '459',\n",
              " 517: 'call',\n",
              " 518: 'design',\n",
              " 519: 'spend',\n",
              " 520: 'hou',\n",
              " 521: 'directli',\n",
              " 522: 'revers',\n",
              " 523: 'belong',\n",
              " 524: '445',\n",
              " 525: '515',\n",
              " 526: '150',\n",
              " 527: '110',\n",
              " 528: 'charg',\n",
              " 529: 'minimum',\n",
              " 530: 'intercontinent',\n",
              " 531: 'final',\n",
              " 532: '823',\n",
              " 533: '1059',\n",
              " 534: '271',\n",
              " 535: 'abl',\n",
              " 536: 'put',\n",
              " 537: 'locat',\n",
              " 538: 'hartfield',\n",
              " 539: '225',\n",
              " 540: '1158',\n",
              " 541: 'equip',\n",
              " 542: 'indiana',\n",
              " 543: '130',\n",
              " 544: 'continu',\n",
              " 545: 'everywher',\n",
              " 546: 'whether',\n",
              " 547: 'usa',\n",
              " 548: 'red',\n",
              " 549: 'eye',\n",
              " 550: '1045',\n",
              " 551: 'current',\n",
              " 552: 'visit',\n",
              " 553: '55',\n",
              " 554: 'determin',\n",
              " 555: 'thing',\n",
              " 556: '705',\n",
              " 557: 'catch',\n",
              " 558: 'straight',\n",
              " 559: '1055',\n",
              " 560: '405',\n",
              " 561: 'equal',\n",
              " 562: 'hope',\n",
              " 563: 'symbol',\n",
              " 564: 'sort',\n",
              " 565: 'cover',\n",
              " 566: '810',\n",
              " 567: 'oper',\n",
              " 568: '1205',\n",
              " 569: 'besid',\n",
              " 570: 'within',\n",
              " 571: \"'ve\",\n",
              " 572: 'got',\n",
              " 573: 'somebodi',\n",
              " 574: 'els',\n",
              " 575: 'level',\n",
              " 576: 'vicin',\n",
              " 577: '311',\n",
              " 578: 'mia',\n",
              " 579: 'instead',\n",
              " 580: 'eleven',\n",
              " 581: 'greatest',\n",
              " 582: 'summer',\n",
              " 583: '300',\n",
              " 584: 'lax',\n",
              " 585: 'econom',\n",
              " 586: 'bay',\n",
              " 587: '402',\n",
              " 588: '767',\n",
              " 589: 'j31',\n",
              " 590: '1020',\n",
              " 591: '730',\n",
              " 592: '400',\n",
              " 593: '1993',\n",
              " 594: 'toward',\n",
              " 595: 'home',\n",
              " 596: '1850',\n",
              " 597: '1505',\n",
              " 598: '723',\n",
              " 599: 'bring',\n",
              " 600: 'yyz',\n",
              " 601: 'non',\n",
              " 602: '500',\n",
              " 603: '428',\n",
              " 604: '98',\n",
              " 605: 'qualifi',\n",
              " 606: 'd10',\n",
              " 607: '539',\n",
              " 608: 'fine',\n",
              " 609: '1200',\n",
              " 610: 'concern',\n",
              " 611: 'iah',\n",
              " 612: '1230',\n",
              " 613: '3357',\n",
              " 614: '323',\n",
              " 615: '229',\n",
              " 616: 'inexpens',\n",
              " 617: 'earlier',\n",
              " 618: '1083',\n",
              " 619: '0900',\n",
              " 620: '645',\n",
              " 621: 'singl',\n",
              " 622: '4400',\n",
              " 623: '1133',\n",
              " 624: '43',\n",
              " 625: 'town',\n",
              " 626: 'variou',\n",
              " 627: '212',\n",
              " 628: '305',\n",
              " 629: 'anyth',\n",
              " 630: '771',\n",
              " 631: 'jet',\n",
              " 632: 'nevada',\n",
              " 633: 'stay',\n",
              " 634: 'order',\n",
              " 635: '497766',\n",
              " 636: 'sound',\n",
              " 637: '1500',\n",
              " 638: '1209',\n",
              " 639: 'sd',\n",
              " 640: '1300',\n",
              " 641: 'enrout',\n",
              " 642: 'advertis',\n",
              " 643: 'seventeen',\n",
              " 644: 'compani',\n",
              " 645: '1940',\n",
              " 646: 'largest',\n",
              " 647: 'oak',\n",
              " 648: 'twelv',\n",
              " 649: '163',\n",
              " 650: 'nighttim',\n",
              " 651: '819',\n",
              " 652: '417',\n",
              " 653: 'work',\n",
              " 654: 'scenario'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwzRRO70Hk3S",
        "colab_type": "text"
      },
      "source": [
        "Then, we pad the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rarC0FhupPMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dae3cd4e-cb5a-4b54-b746-dc5eb6665de5"
      },
      "source": [
        "len(tokenized_train_data), len(tokenized_test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4834, 800)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EwIILkjp-lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len_list = []\n",
        "for sent in tokenized_train_data:\n",
        "  len_sent = len(sent)\n",
        "  len_list.append(len_sent)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKDmIAGeqQ71",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4782773-e0e1-4801-e3e7-4ec3def07f44"
      },
      "source": [
        "np.percentile(np.array(len_list),99.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z_dRUZay3KD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We use pad_sequences from tensorflow.keras.preprocessing.sequence library\n",
        "train_data= pad_sequences(tokenized_train_data, maxlen= 20, padding= \"pre\")\n",
        "test_data= pad_sequences(tokenized_test_data, maxlen= 20, padding= \"pre\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d86kycByrJBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e75bc9b-0127-4933-88fe-d86834c3957b"
      },
      "source": [
        "train_data.shape, test_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4834, 20), (800, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu5TP7HGyJww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIOlvbZgyft1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d683ddaf-9f9c-4649-d27e-810f15ce1ebe"
      },
      "source": [
        "vocab_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "654"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyINFsVPz7wZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[655*100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8JAGP7bwA0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Learn your own embeddings\n",
        "inputs = Input(name='inputs',shape=[max_len]) #(20,)\n",
        "\n",
        "layer1 = Embedding(vocab_len+1,300,input_length=max_len,\n",
        "                  mask_zero=True)(inputs) #(20,100)\n",
        "layer2 = LSTM(64)(layer1) #(64,)\n",
        "layer3 = Dense(256,name='FC1')(layer2) #(256,)\n",
        "layer4 = Activation('relu')(layer3)\n",
        "layer5 = Dropout(0.5)(layer4)\n",
        "layer6 = Dense(8,name='out_layer')(layer5) #(8,)\n",
        "layer7 = Activation('softmax')(layer6)\n",
        "model = Model(inputs=inputs,outputs=layer7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXF5mB1q4xw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "db278912-debc-4dd0-f47a-d3874c2c4bad"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, 20)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 20, 300)           196500    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "FC1 (Dense)                  (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "out_layer (Dense)            (None, 8)                 2056      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 308,636\n",
            "Trainable params: 308,636\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85Qz3HAQ0FuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "cb32334c-1feb-4451-a2d6-f964fa0b001f"
      },
      "source": [
        "##Use pretrained embeddings\n",
        "!wget http://vectors.nlpl.eu/repository/20/0.zip\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-25 16:34:28--  http://vectors.nlpl.eu/repository/20/0.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 344050746 (328M) [application/zip]\n",
            "Saving to: 0.zip\n",
            "\n",
            "0.zip               100%[===================>] 328.11M  78.6MB/s    in 5.2s    \n",
            "\n",
            "2020-07-25 16:34:33 (63.2 MB/s) - 0.zip saved [344050746/344050746]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3YxTijU06xR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "d26b552a-ad8a-4291-c42e-bebda6e2927a"
      },
      "source": [
        "!unzip 0.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  0.zip\n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLyRvllt1B8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "5d72a3a8-3ec6-4d8e-9b76-da2d82d4a784"
      },
      "source": [
        "!head -10 model.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "163473 300\n",
            "say_VERB -0.008861 0.097097 0.100236 0.070044 -0.079279 0.000923 -0.012829 0.064301 -0.029405 -0.009858 -0.017753 0.063115 0.033623 0.019805 0.052704 -0.100458 0.089387 -0.040792 -0.088936 0.110212 -0.044749 0.077675 -0.017062 -0.063745 -0.009502 -0.079371 0.066952 -0.070209 0.063761 -0.038194 -0.046252 0.049983 -0.094985 -0.086341 0.024665 -0.112857 -0.038358 -0.007008 -0.010063 -0.000183 0.068841 0.024942 -0.042561 -0.044576 0.010776 0.006323 0.088285 -0.062522 0.028216 0.088291 0.033231 -0.033732 -0.002995 0.118994 0.000453 0.158588 -0.044475 -0.137629 0.066080 0.062824 -0.128369 -0.087959 0.028080 0.070063 0.046700 -0.083278 -0.118428 0.071118 0.100757 0.017944 0.026296 0.017282 -0.082127 -0.006148 0.002967 -0.032857 -0.076493 -0.072842 -0.055179 -0.081703 0.011437 -0.038698 -0.062540 -0.027899 0.087635 0.031870 0.029164 0.000524 -0.039895 -0.055559 0.024582 -0.030595 0.003942 -0.034500 0.003012 -0.023863 0.033831 0.061476 -0.090183 -0.039206 -0.026586 -0.042763 0.049835 -0.052496 -0.020044 0.073703 0.096775 0.033063 0.000313 -0.022581 -0.141154 0.032095 0.077733 -0.063739 -0.055647 -0.017604 0.044639 -0.062925 -0.001960 0.024665 -0.009416 -0.021381 0.082724 -0.031026 0.027255 0.066198 0.000845 0.008393 0.039434 0.054104 -0.060255 0.034266 0.079435 0.043624 -0.015871 -0.038030 -0.030374 -0.020542 0.007132 0.008708 0.087840 0.017351 -0.089493 0.030182 0.026961 -0.071212 -0.004854 0.007389 0.067203 -0.026351 -0.011460 -0.058723 0.013153 -0.020313 -0.051170 0.002242 0.088222 -0.004267 -0.073523 -0.021874 -0.033585 -0.048553 -0.019119 -0.025310 0.053096 0.111063 0.035042 -0.082811 -0.073749 -0.010048 0.012265 -0.023893 -0.125340 0.026611 0.043258 -0.010473 -0.044428 -0.039251 -0.046891 -0.013008 0.062219 0.078732 -0.086303 0.016901 0.010331 -0.043754 -0.057733 -0.037964 0.024907 0.068143 -0.019992 -0.035030 0.038854 0.034345 -0.048839 -0.105419 0.043013 -0.023374 -0.077629 -0.076465 0.078564 -0.024519 0.041293 -0.032088 -0.007053 0.022618 -0.004657 -0.093970 -0.000199 0.004813 -0.044789 -0.127900 -0.033516 -0.043816 0.033056 -0.057619 0.004901 0.018863 0.039752 0.000739 -0.136350 -0.067819 -0.014856 0.058351 -0.014275 -0.000873 -0.039388 -0.017191 -0.051184 -0.046863 0.006143 -0.075998 -0.064695 0.046676 -0.020558 0.082474 0.160449 -0.027475 0.009541 -0.021876 0.027416 0.078049 0.089309 0.032928 -0.033272 0.048905 0.061164 0.054811 0.024527 -0.034978 -0.018083 -0.077601 0.034112 -0.021121 0.098856 0.019585 -0.058928 -0.016126 -0.011748 0.031588 0.003205 -0.077483 -0.002372 -0.113548 0.047445 -0.027094 -0.032843 0.042378 -0.074703 0.057001 0.012020 0.131156 0.002080 -0.065770 0.112443 0.047786 0.024492 -0.108401 0.016836 0.001478 0.041542 -0.067801 0.102876 -0.052808 -0.136035 0.073852 0.079966 -0.000586 0.034055 -0.053040 0.050461 -0.021550 0.014827 0.077605 -0.024783 -0.082388 0.074410 -0.033689 -0.010982 0.043733\n",
            "go_VERB 0.010490 0.094733 0.143699 0.040344 -0.103710 -0.000016 -0.014351 0.019653 0.069472 -0.046938 -0.057882 0.076405 -0.025230 0.026663 0.029986 -0.001605 -0.027803 0.037521 -0.050608 0.016215 0.025947 0.061172 -0.037448 -0.079232 0.071731 -0.085143 0.021494 -0.135554 -0.026115 -0.066408 0.022858 0.083231 0.020998 -0.049906 -0.079992 -0.060827 -0.028916 -0.029005 0.026067 -0.074869 0.073802 0.023593 -0.024348 -0.093236 0.006169 0.013119 0.007817 -0.088096 -0.012373 0.099807 0.011438 0.028583 0.025614 0.175403 0.007033 0.038856 0.004040 -0.088907 0.079697 0.037448 -0.128230 -0.066502 -0.018969 0.025777 0.035905 0.003710 -0.089079 0.071521 0.039237 0.052136 0.020986 -0.030793 -0.069486 -0.137115 0.008305 0.020813 -0.155342 0.000619 -0.033499 -0.104162 -0.061528 -0.043877 -0.042524 -0.032872 0.045071 0.072908 0.096057 0.141987 -0.078056 -0.013102 -0.026589 -0.073783 0.114807 0.077389 -0.041879 -0.052886 0.053710 0.036806 -0.035973 0.049071 -0.107199 -0.043581 0.016515 -0.029278 -0.026228 0.068037 -0.024183 0.040984 -0.020469 -0.103833 -0.007225 -0.073788 -0.051063 -0.037850 0.052581 -0.053090 -0.012198 -0.057343 0.024050 -0.046498 0.003065 -0.058912 0.043695 0.006340 0.060953 -0.008608 -0.029686 0.081187 -0.020058 0.059240 -0.061306 -0.002190 -0.020671 0.076712 0.049087 0.001153 0.087481 0.008559 0.069936 -0.015886 0.006122 0.038000 -0.071984 0.005263 0.060463 -0.051217 -0.034060 0.045217 0.059163 -0.048462 -0.005371 0.009663 0.081303 0.051019 -0.001248 -0.022637 0.016228 -0.006395 -0.053985 -0.014513 -0.017219 -0.010658 -0.012446 -0.035279 -0.003882 0.036453 0.029681 0.021278 0.006188 0.027861 0.076864 -0.042835 -0.022834 0.013928 0.066150 0.040982 -0.110985 -0.018865 0.006675 0.019173 0.021484 -0.021977 -0.035462 0.000464 -0.024281 0.010881 -0.064037 -0.024893 -0.095968 0.020834 -0.114225 -0.023433 -0.043971 0.014273 0.013481 -0.007542 0.079197 0.021280 -0.129871 0.080770 0.028912 -0.044134 -0.019904 -0.039406 -0.076024 0.058488 -0.094331 -0.082633 0.017676 -0.084006 -0.024444 -0.049778 -0.044615 -0.013499 -0.036736 -0.038579 -0.117319 0.012026 -0.007846 0.024003 -0.101645 0.111720 -0.010241 0.050279 -0.002212 0.060056 -0.116837 0.006078 -0.017954 -0.021794 0.020252 -0.031337 -0.032407 0.081086 -0.095125 0.041699 0.015953 -0.045653 -0.022522 -0.021422 -0.029167 0.052594 0.016523 0.081598 -0.027877 0.000609 0.012837 0.011880 0.074220 0.009736 0.006465 -0.140252 0.010762 -0.038319 0.038924 0.042537 0.005027 0.014024 0.024548 0.050131 -0.048069 -0.012616 -0.052162 -0.100378 0.067741 -0.067824 -0.020692 -0.043022 -0.038036 -0.016860 0.027835 0.140990 -0.045201 -0.069347 0.174518 -0.000236 0.008150 -0.039823 0.041197 0.056322 0.085883 0.027376 0.036537 0.094723 -0.103076 0.105746 0.059074 0.010947 0.099756 -0.027213 0.128793 -0.054593 0.025890 0.053512 0.005200 -0.035256 0.063273 -0.027069 0.046354 -0.002262\n",
            "make_VERB -0.013029 0.038892 0.008581 0.056925 -0.100181 0.011566 -0.072478 0.156239 0.038442 -0.073817 -0.000439 0.114153 -0.051814 -0.056424 -0.038872 0.054174 0.000059 0.039477 -0.021345 0.053860 -0.131669 -0.020844 0.012362 -0.016145 0.048171 -0.122080 0.028292 -0.043984 -0.025178 -0.006927 -0.029133 -0.085539 -0.086455 0.001830 -0.099361 -0.029536 0.071144 -0.003143 0.027941 -0.035858 0.026530 0.004768 0.021307 -0.065139 -0.053572 0.038951 0.045786 -0.045258 -0.037586 0.038983 -0.062755 -0.000504 0.044502 0.123845 -0.050279 0.030425 -0.067798 -0.037958 0.023805 -0.011021 -0.041084 -0.090643 0.130500 0.046460 -0.040764 0.020988 -0.087054 -0.017896 0.056193 0.007352 -0.019590 -0.048728 -0.027895 -0.027241 -0.038715 0.008038 -0.172688 -0.106911 -0.012085 -0.050829 -0.053590 -0.059879 -0.030488 -0.025220 0.020381 0.102120 0.041989 0.119341 -0.006702 0.035009 0.016077 -0.014298 0.124971 0.050049 0.113425 -0.027587 -0.001379 -0.031188 0.041054 -0.013872 -0.134232 -0.073757 0.075578 -0.064260 0.035823 0.032695 -0.059019 0.086900 -0.049042 -0.105385 -0.024058 0.095202 -0.044429 -0.053781 -0.013759 -0.077265 -0.043720 -0.082217 0.128089 -0.041757 -0.023743 0.027764 0.008487 -0.022274 -0.023357 -0.013653 0.047372 0.098364 -0.020791 -0.063818 0.055996 -0.007599 0.018954 -0.003601 0.055991 -0.089158 0.008229 -0.027915 0.056351 0.101133 0.043454 0.026218 0.010540 0.053571 0.079725 -0.048278 -0.048708 -0.075923 -0.045807 0.083970 -0.087983 0.058780 0.025992 -0.008407 -0.059681 -0.022862 0.099799 0.083928 -0.024096 0.008313 -0.065932 -0.003852 0.051210 -0.104068 -0.029864 0.021315 -0.036515 -0.050546 0.003077 0.007452 -0.020468 0.035296 -0.025792 -0.045913 0.042664 -0.025302 -0.057182 -0.026525 -0.053029 -0.009697 0.031003 0.064251 -0.096399 -0.020674 0.006306 -0.004981 -0.118857 -0.058013 -0.018890 0.042343 -0.111604 -0.071149 0.042898 0.094869 -0.029797 -0.134403 -0.030753 0.050269 -0.096115 0.019021 0.014348 -0.049818 -0.017920 0.044926 0.038627 -0.091947 -0.001567 0.064930 -0.065977 -0.015673 0.034979 0.064560 0.036580 -0.000075 -0.064665 -0.054986 -0.090783 -0.033908 0.106271 0.058234 -0.100301 0.015398 -0.072886 0.019940 0.066563 0.063845 -0.036548 -0.018204 -0.008618 0.098109 -0.128401 -0.053501 -0.032671 0.027777 -0.043889 -0.018033 0.099028 -0.026501 -0.026575 -0.106259 0.036872 0.024990 0.003347 0.045086 -0.083903 0.021039 0.056445 -0.053898 0.011539 -0.033661 0.020421 -0.051413 0.021900 0.075706 0.089103 -0.022953 -0.032130 -0.049067 0.014476 -0.036070 0.010638 -0.049193 -0.005560 -0.094642 -0.045530 -0.010048 0.074026 0.053386 -0.006803 0.043264 -0.004896 0.020676 0.002030 0.019262 0.043679 -0.006854 -0.064545 -0.059780 -0.070871 0.004817 0.058769 -0.052450 -0.023481 -0.036496 -0.029701 -0.002672 -0.029965 0.053667 0.038260 -0.026692 0.068764 -0.070122 0.060288 0.124118 -0.064670 -0.044363 0.023818 -0.022746 -0.086708 0.016196\n",
            "get_VERB 0.019242 0.144838 0.155635 0.009607 -0.169437 -0.004972 -0.021559 0.009400 0.074920 -0.033244 -0.032937 0.112560 0.041283 -0.030355 -0.048271 -0.061402 0.048208 0.083419 -0.043215 0.069025 -0.027292 0.097641 -0.070595 -0.034194 0.091538 -0.068585 0.012530 -0.120053 -0.014222 0.002379 0.070677 0.015263 0.030467 -0.001756 -0.013990 -0.026711 0.036041 0.014917 0.031644 -0.055844 0.115340 -0.003877 -0.045724 -0.025892 -0.024716 0.020095 -0.024788 0.005623 0.055026 0.078559 0.011337 0.033604 0.051766 0.135975 0.061593 0.029842 -0.021268 -0.136608 0.079957 0.011375 -0.155011 -0.151481 0.076298 0.031386 0.020274 0.028823 -0.127225 0.016972 0.000477 0.005670 -0.015052 -0.032207 -0.054631 -0.125453 0.027146 0.026122 -0.025028 -0.046540 0.021146 -0.082786 -0.051401 -0.015178 -0.017908 0.031622 0.061065 0.053762 0.093668 0.120508 0.013383 -0.019997 0.047220 -0.039273 0.070729 0.006117 0.033016 -0.059557 0.080700 0.044238 -0.013197 0.072017 -0.049897 -0.035379 -0.028077 -0.068918 0.048224 0.093904 -0.011614 0.143074 -0.066022 -0.104345 -0.080770 -0.034034 0.019643 -0.048965 0.020702 -0.061310 -0.077258 -0.091273 0.063781 -0.111229 0.058853 -0.006970 0.029294 0.001331 0.057966 -0.035211 0.033447 0.015224 0.027628 0.020672 0.023872 0.007849 -0.001215 0.045739 -0.032844 -0.027810 0.047459 0.011252 0.034142 -0.020341 0.063802 -0.005105 -0.014244 0.015935 -0.008431 -0.028795 -0.043172 -0.003883 0.023328 -0.022711 0.001897 0.032548 0.064574 0.097152 0.003275 -0.109298 0.024141 0.090362 -0.038664 -0.023928 -0.006557 0.025754 -0.011571 -0.053434 0.040903 0.061501 0.144468 0.017326 -0.032445 -0.024866 -0.000533 -0.067980 -0.099827 0.011754 0.026172 0.041204 -0.059723 0.026298 0.006623 -0.030971 0.030617 -0.008276 -0.084025 -0.030169 0.044463 0.002809 -0.030988 -0.025040 -0.059924 0.042590 -0.041501 0.018392 -0.107077 0.040232 -0.015956 -0.034321 0.064614 0.023561 -0.118772 0.011774 0.053385 -0.059752 -0.007313 -0.024684 0.012329 0.024288 -0.019210 -0.043125 0.031520 -0.072438 -0.043097 -0.061750 -0.049240 -0.039941 -0.086947 -0.019136 -0.082013 -0.095680 -0.012216 0.044958 -0.083804 -0.020841 -0.024199 0.085375 -0.000988 0.002353 -0.075649 -0.016678 -0.042220 0.002328 0.046584 -0.053008 -0.002773 0.059518 -0.113334 0.082102 0.038316 -0.023807 0.014160 -0.024084 0.049738 0.003309 0.020473 0.056583 -0.040877 -0.036386 0.033831 0.014504 0.005588 -0.098602 0.013935 -0.115838 0.048181 -0.013819 0.030253 -0.035629 0.022863 -0.019994 -0.016116 -0.052284 -0.034443 0.024592 -0.028994 -0.093012 -0.007058 0.013011 0.000991 -0.014438 -0.044545 0.040938 -0.043503 0.103244 -0.090978 0.005134 0.156962 0.013555 0.053622 -0.059002 0.032924 0.010204 -0.054882 -0.070490 0.102098 0.071841 -0.041202 0.079912 0.031834 0.048141 0.104557 -0.010763 0.057594 -0.091408 0.010093 0.010459 0.074382 0.028358 0.016023 -0.091680 0.031687 -0.108943\n",
            "one_NUM 0.056419 -0.021141 0.090616 -0.032564 -0.054807 0.031836 -0.004311 0.064928 -0.033537 0.008632 0.052463 0.034665 0.016636 -0.006993 0.002519 -0.049758 0.003396 0.076139 -0.040574 -0.034938 -0.018776 0.016026 -0.049461 -0.058605 0.052319 -0.022392 0.037263 -0.101570 -0.015736 0.014700 0.005736 -0.098316 0.023485 0.079696 0.000523 -0.072989 0.112214 -0.042671 -0.039667 -0.048063 0.076580 -0.044722 -0.009309 -0.029152 -0.032665 -0.017058 0.066964 -0.003375 0.010473 0.053499 -0.057250 0.022080 0.031383 0.110925 -0.070559 0.090881 0.004533 -0.072508 0.064684 0.078511 -0.096904 -0.011367 0.023967 0.045979 0.047333 -0.058862 -0.043411 0.133778 -0.040768 -0.027242 -0.013070 -0.021536 0.068636 -0.093734 -0.071668 0.021352 0.002289 0.037268 -0.065373 -0.020971 -0.023998 -0.035441 0.033266 -0.028960 0.040619 0.064281 0.065757 0.123596 0.082568 -0.004874 0.028938 0.047771 0.025916 0.021118 -0.023998 0.009172 0.100679 -0.078543 -0.046099 0.037856 -0.161992 -0.039157 0.007258 -0.039263 0.063678 0.060994 0.025443 0.036222 -0.007018 -0.108600 0.022631 -0.031306 -0.050116 0.006508 -0.018496 -0.098152 -0.043395 -0.012609 0.047591 -0.006303 0.029529 0.003471 0.004601 -0.050615 0.014272 0.049927 0.105728 0.021712 -0.030996 0.026857 -0.060846 -0.107893 0.017269 0.011160 0.020126 -0.115469 -0.015772 -0.007356 0.083882 0.119524 0.075473 -0.050709 0.036562 -0.059548 -0.053619 -0.049486 -0.124977 0.004016 -0.001549 0.007669 -0.039740 0.002400 -0.069183 -0.065801 -0.088064 -0.107159 0.072443 0.083424 -0.030178 -0.033223 0.084401 0.044171 0.013811 -0.084562 0.003194 0.056247 -0.022866 0.023806 0.047635 0.025468 0.069964 -0.096787 -0.025001 -0.021526 0.061188 0.045733 -0.099350 0.016400 -0.084030 0.056672 0.099467 -0.052893 -0.100382 0.124380 0.018206 0.034541 -0.014580 -0.124287 -0.103852 0.053850 -0.017001 -0.062173 -0.063301 0.024652 -0.023819 -0.057063 -0.013738 -0.027791 -0.013030 0.030126 -0.000573 -0.026613 -0.005942 0.057634 -0.079672 0.022556 0.000011 -0.037800 0.074457 -0.071388 0.086022 0.013307 -0.035879 -0.072396 0.034418 0.004146 -0.011763 0.010722 -0.015462 0.070248 -0.097890 -0.033176 -0.078940 0.169393 0.050572 0.045639 0.004937 0.008214 -0.024237 -0.039713 -0.082674 -0.056171 -0.114638 -0.037084 -0.019666 0.023100 0.091380 -0.070856 0.019748 0.032893 0.054247 0.001259 0.166033 -0.021817 -0.076404 -0.046031 0.002690 -0.015014 0.087486 -0.147485 0.035263 -0.095335 -0.035910 -0.060311 0.068841 0.034646 -0.004976 -0.025316 -0.066141 0.042181 -0.001685 -0.061911 -0.060588 -0.036907 -0.003193 -0.042462 -0.023907 0.015987 -0.030946 0.041203 0.003092 0.075626 0.082451 -0.077289 0.102090 -0.026878 0.057229 -0.067434 -0.041582 0.036640 0.022304 0.018005 -0.005375 0.038557 -0.087250 0.042553 -0.052500 0.070422 0.079278 -0.017086 0.055328 0.025901 0.111785 0.057416 0.022670 0.048453 -0.034165 -0.034901 0.014241 0.061734\n",
            "see_VERB 0.033784 -0.033085 0.020113 -0.010017 -0.081187 -0.021287 -0.009972 0.037831 -0.001592 0.079325 0.025121 -0.012038 -0.060439 0.023697 -0.011899 -0.009593 0.043340 -0.054792 -0.058171 0.111542 -0.072478 0.046219 -0.036414 -0.038994 0.010711 -0.022551 0.051247 -0.030018 -0.015760 -0.022421 0.029474 0.033584 0.077213 0.026976 -0.026057 -0.024171 -0.060402 -0.022088 0.036667 0.050913 0.044288 0.050425 0.020312 -0.079651 0.039995 0.025213 -0.093921 0.123022 -0.018795 -0.019389 0.002286 0.001979 -0.101728 0.107971 -0.068966 0.035741 0.015628 -0.035574 0.153279 0.094290 -0.104906 -0.052320 0.123525 0.008491 -0.095255 -0.007215 0.046057 0.073950 0.000288 0.048581 0.015934 0.018141 -0.025546 -0.197751 -0.042662 -0.052096 -0.026476 -0.062207 0.041462 -0.126658 -0.054443 0.032953 -0.042276 -0.002271 0.064819 -0.000886 0.057284 0.151726 -0.005298 0.027951 -0.030457 -0.006905 -0.063591 0.038784 0.042528 0.009519 -0.006915 0.012526 -0.026987 -0.031367 -0.032849 -0.069462 0.078398 -0.077442 0.025363 0.015721 0.006499 0.050169 0.013559 -0.041564 -0.021559 0.038734 -0.048881 -0.014219 0.018850 -0.076032 -0.031383 -0.053150 0.073602 -0.060924 0.059121 -0.135463 -0.065702 0.017648 0.056463 -0.019903 0.036354 0.063048 0.043614 -0.058998 0.036254 -0.089223 -0.017289 -0.016416 0.103582 -0.060645 -0.003059 -0.069493 0.018295 -0.066589 0.049994 0.053174 -0.001443 -0.069640 0.044240 -0.050163 -0.073309 0.004212 0.096908 0.120702 -0.067013 -0.009905 -0.043493 -0.076316 0.014215 -0.031754 -0.021288 -0.004769 -0.137067 -0.024747 -0.031678 0.015084 0.031505 -0.103574 -0.066272 0.042509 0.060396 -0.059968 0.002428 -0.024155 0.063192 -0.077420 -0.094139 0.057205 0.022803 0.069838 -0.001844 0.020190 -0.049075 -0.048242 0.025786 0.067670 -0.152904 0.038227 0.090410 -0.012939 -0.040886 -0.011291 0.012498 -0.020443 -0.048081 0.018145 0.055328 0.065196 -0.062635 -0.106917 0.064730 0.044867 -0.068668 -0.037746 0.057211 -0.034426 0.051337 -0.025572 0.046295 -0.011716 -0.006654 -0.030194 -0.051332 0.026101 0.104883 -0.007145 -0.070217 -0.000867 0.002503 -0.064973 0.037842 0.021236 0.023925 -0.017935 -0.079966 0.055120 -0.034364 0.071393 -0.029756 0.055380 -0.002203 0.004057 0.018498 0.023873 -0.042996 -0.027251 -0.031769 -0.023764 -0.056463 -0.019280 0.102642 -0.095517 0.071215 0.080328 -0.003316 0.068579 0.052600 0.053909 0.108883 0.012581 0.043106 0.031879 0.054328 -0.030787 -0.028546 -0.068817 -0.013480 0.036747 0.036505 -0.026683 0.034958 0.075650 0.001530 0.077361 -0.012978 -0.059580 -0.003257 -0.075410 0.026864 -0.090516 0.148559 -0.067887 -0.009229 0.107859 0.011224 0.026525 -0.020502 0.029137 0.059563 0.094475 0.072240 -0.095192 0.006081 0.021361 0.136805 -0.011188 -0.004593 0.007071 -0.166692 0.081480 0.042853 -0.013095 0.103005 0.036355 -0.033840 -0.017550 0.021768 0.056411 -0.041009 -0.057657 0.068774 0.006003 0.083118 -0.005328\n",
            "time_NOUN 0.028551 -0.088598 0.050082 0.007181 -0.104172 0.101837 0.035492 -0.016076 -0.036470 -0.031597 0.008517 0.081070 0.005958 0.011227 0.055194 -0.085483 -0.028711 -0.041737 -0.087045 0.040034 -0.014075 0.115195 0.014005 -0.067294 0.107071 -0.100702 -0.009378 -0.124226 0.017455 0.003322 0.005558 -0.061389 0.162625 -0.079217 -0.014027 0.008225 0.009872 0.019838 0.037906 -0.081853 0.025483 0.006533 -0.027086 -0.061254 0.021571 -0.058520 -0.030755 -0.037765 0.013022 0.099744 -0.015871 -0.040148 0.013853 0.174780 -0.030214 0.036497 -0.030496 -0.072386 0.129520 -0.039360 -0.012611 -0.044570 0.009870 0.027195 -0.018254 -0.064432 -0.027389 0.076080 0.017096 0.094141 -0.004854 -0.119384 -0.079426 -0.096194 -0.061261 -0.046198 -0.066695 -0.095495 0.071205 -0.035128 -0.109664 0.000560 -0.008151 0.001801 0.033679 0.077297 0.069809 0.126302 -0.106770 0.054766 -0.059580 -0.031521 0.100238 0.050529 -0.093611 0.014521 0.113995 0.047877 0.043517 0.046725 -0.064180 -0.012568 -0.001566 -0.058337 0.033448 0.017877 -0.060126 0.057385 -0.074544 0.033809 -0.063151 0.051488 -0.082470 0.012745 -0.012867 -0.022761 -0.073556 -0.064041 0.072679 0.004931 0.033317 -0.036562 0.097679 0.064440 0.006349 0.029687 0.079091 0.126180 0.017304 -0.055640 -0.004360 -0.071465 0.020999 0.020292 0.020851 -0.059998 0.026812 -0.025640 0.056559 0.062663 0.068633 0.003051 0.038869 -0.033266 -0.003146 -0.004676 -0.060268 -0.099329 -0.011529 -0.024929 0.052014 0.016963 0.007527 -0.074523 -0.107231 -0.095747 0.040862 0.055063 0.037597 0.030260 -0.060898 -0.035649 0.030211 -0.061037 -0.030174 0.025636 0.006325 -0.005351 -0.055750 0.042350 0.005378 -0.067135 -0.089138 -0.024828 -0.011560 0.063750 -0.050910 -0.029080 -0.003223 0.070240 -0.015769 0.085594 -0.091409 0.066143 -0.005699 -0.074672 0.015766 -0.008242 -0.016767 0.064410 -0.070908 -0.074450 0.005181 0.062501 -0.007251 -0.036160 -0.030252 -0.006774 -0.117676 0.085261 0.055248 0.023099 -0.001916 0.010082 0.004036 0.009452 0.009258 0.015016 0.040363 0.048659 0.021832 -0.100119 -0.093323 0.046102 0.003035 -0.010301 0.019116 0.004175 0.018846 0.026686 -0.200220 0.021341 -0.039848 0.050640 -0.026792 0.057337 -0.060018 0.013275 -0.043792 -0.020855 0.018315 -0.052217 -0.129547 -0.029040 -0.041891 -0.064859 -0.007379 -0.078831 -0.039299 -0.070625 -0.035255 0.058762 0.083695 0.085014 -0.052283 -0.005346 0.002431 0.030622 0.076632 -0.101137 0.037876 -0.031314 0.000538 -0.022329 0.090587 0.060532 -0.022718 -0.007348 0.040430 0.084318 -0.037918 -0.041699 0.064522 -0.004134 -0.056841 0.021044 0.019699 -0.000656 -0.051270 -0.011215 -0.010951 -0.015126 -0.095848 -0.043651 0.068587 -0.033160 0.112103 -0.069295 0.069698 -0.027304 -0.032045 0.011533 0.078525 0.012872 -0.072700 -0.018674 -0.048350 0.033240 0.006589 0.038484 0.049222 -0.036532 0.050987 0.054671 -0.073558 -0.075419 -0.051517 -0.082573 -0.022406 0.042652\n",
            "take_VERB -0.085915 0.072050 0.061177 0.034385 -0.130897 -0.020019 0.041692 -0.001646 -0.000336 -0.085338 0.038205 0.018875 -0.083286 -0.061232 0.016179 0.066189 0.060670 0.000543 -0.135004 0.049699 -0.012316 -0.007948 -0.024639 -0.000349 -0.021084 -0.054148 0.061099 -0.111185 0.005860 -0.023507 0.029399 0.055637 0.049720 0.019389 0.024483 0.000545 -0.015728 -0.084071 -0.045200 -0.039850 -0.057122 -0.043768 -0.087916 -0.003074 0.094422 0.002184 0.054202 -0.081253 0.060053 0.047244 0.096669 0.020689 0.049919 0.160756 0.074185 0.068677 0.081737 0.046915 0.130613 0.003062 -0.094222 -0.038692 0.007875 0.048645 0.055408 -0.037991 -0.064129 0.057689 0.079305 0.059282 0.004156 0.006521 0.049386 -0.046141 -0.058870 -0.012268 -0.029792 -0.073071 -0.006719 -0.020783 -0.053037 -0.029306 -0.123163 -0.016259 -0.005775 -0.005656 0.082892 0.182866 -0.053775 0.081097 0.031257 -0.082803 0.072860 0.058470 -0.042368 -0.044028 -0.007983 0.027884 0.029243 -0.005043 -0.027391 -0.081547 0.087199 -0.054481 0.092473 0.073842 0.080296 0.116464 -0.060440 -0.112892 0.000785 0.031157 -0.055366 -0.042485 -0.029086 -0.058525 0.026030 -0.063302 0.052599 -0.005851 0.048085 -0.056849 -0.002640 0.003858 -0.009217 0.042292 0.014251 0.111839 0.003789 0.004826 -0.055015 0.037018 0.051596 0.034754 0.054819 -0.125969 -0.021572 -0.046395 0.050502 0.015387 0.046864 0.004564 0.036650 0.010489 0.024583 -0.036233 -0.043142 -0.063384 -0.008670 0.051395 0.047257 -0.015780 0.067077 -0.027996 -0.014117 -0.058627 0.018429 0.073644 0.050070 0.010536 0.027303 -0.002755 0.026361 -0.045678 -0.013685 0.031982 -0.006479 0.034540 -0.003912 0.057480 -0.007845 -0.097392 0.028641 -0.050393 0.085079 0.011244 -0.097678 0.007091 -0.027963 0.022363 0.002283 0.037801 -0.037196 0.169746 -0.038624 0.078767 -0.001875 0.054219 -0.052247 0.026505 -0.138177 -0.021248 -0.073399 0.058894 -0.001102 -0.008011 -0.044078 -0.001408 -0.078455 0.096208 -0.010742 -0.043702 0.079016 -0.018590 0.013306 0.022266 0.032821 0.004880 -0.002337 0.045593 -0.024818 -0.007891 0.008684 -0.061167 -0.045740 -0.051571 -0.049655 -0.008531 -0.006200 0.048838 -0.072153 0.049921 -0.059215 0.127064 0.040236 0.098747 -0.042659 -0.028465 -0.056385 0.065828 0.036884 -0.096360 0.006464 0.006201 0.056877 0.071954 0.114779 -0.087398 0.008548 0.044174 0.007840 -0.017414 0.046436 0.096034 -0.127091 -0.028683 -0.013463 -0.020997 -0.010382 -0.030867 0.037462 -0.108436 0.018027 -0.034779 0.058547 -0.068791 0.021402 -0.011831 0.002013 0.060412 -0.078119 -0.019209 0.013182 -0.094492 0.110161 -0.063736 0.046850 0.022188 0.007599 -0.007369 -0.058936 0.056017 -0.049103 -0.057295 0.058677 0.147318 0.107422 -0.079093 -0.010623 0.073036 0.024813 -0.045329 0.095850 0.046119 -0.086194 0.039533 -0.001355 0.052867 0.066449 -0.062630 0.020811 -0.032284 -0.087716 0.034099 -0.028051 -0.021583 -0.030066 -0.009957 0.022330 0.032838\n",
            "know_VERB -0.025984 0.067260 0.034687 0.040777 -0.161350 0.013020 -0.007056 -0.024733 0.058577 0.032559 -0.012518 0.108795 0.025453 -0.025084 -0.102243 -0.040177 -0.021312 -0.043816 -0.081668 0.091292 0.038419 0.055116 0.035742 -0.050125 -0.018741 -0.049817 0.012086 -0.079459 -0.000711 -0.023625 0.013238 0.074507 0.030027 -0.001057 0.035990 -0.050576 -0.003847 0.060832 0.011826 0.008473 -0.015987 -0.049248 -0.043038 -0.075997 0.000403 0.001103 -0.003824 -0.029661 0.098314 0.062112 -0.109037 -0.013936 -0.008971 0.112062 -0.019038 -0.048354 0.006206 -0.070438 0.065072 0.018215 -0.104006 -0.086250 0.071208 0.083047 -0.001287 -0.120081 -0.014862 0.129383 0.042387 -0.065375 0.006762 -0.073221 -0.010304 -0.115515 0.007240 0.022477 -0.058240 -0.010925 -0.000522 -0.142643 0.035225 0.062995 -0.036440 -0.023326 0.043814 0.082391 0.105983 0.029809 -0.019996 0.034061 0.027599 -0.003354 -0.002728 0.053269 -0.012316 -0.085819 0.030923 0.038884 -0.022784 0.034281 -0.092045 -0.079878 -0.006614 -0.093812 0.035342 0.094636 -0.057155 0.051942 0.024737 -0.080949 -0.065721 -0.039007 -0.005870 -0.073761 0.076176 -0.017809 0.005946 -0.080602 0.032073 -0.102764 0.038929 -0.075056 -0.013560 -0.018190 0.069482 0.083503 0.027585 0.023521 0.014033 -0.036928 -0.005933 0.030368 0.006602 0.030084 0.044758 -0.018216 0.035520 -0.012898 0.045371 -0.001641 0.110703 0.017023 0.005551 0.045749 -0.005427 -0.074813 -0.010974 -0.010148 0.095004 0.013014 -0.005125 0.016952 -0.030893 -0.062859 -0.080818 -0.051509 0.006347 0.010265 -0.063437 0.009479 0.009224 0.006066 -0.008744 -0.104529 -0.025839 0.070249 0.095893 -0.047559 0.013237 0.055287 0.009931 -0.023470 -0.098192 -0.030587 0.049573 -0.017828 -0.071414 -0.018803 -0.014656 -0.002473 0.036026 0.041413 -0.054189 0.018682 0.084050 -0.029306 -0.026105 -0.016993 -0.026153 0.102438 -0.018404 -0.094386 0.000044 0.060043 -0.082400 -0.121982 -0.031880 -0.045073 -0.080735 0.012308 0.069106 -0.024642 0.040432 0.055083 -0.017416 0.011888 -0.057825 -0.112030 -0.016259 0.037437 0.081257 -0.035797 -0.070929 -0.093251 -0.083276 -0.047928 -0.043073 -0.057629 0.043480 0.033591 -0.152529 0.020240 0.020165 0.082562 -0.033501 0.070734 -0.085496 0.029878 -0.066357 0.026343 -0.050701 -0.037989 -0.134138 0.017599 -0.078392 0.056217 -0.005646 0.014984 0.065454 -0.014985 -0.032853 0.037999 0.024869 0.056706 0.094115 0.001015 0.021370 0.050369 0.088182 -0.045817 -0.098003 -0.092237 0.017012 -0.017444 0.094234 -0.003846 -0.046259 -0.039507 0.024746 0.046465 -0.040855 0.020798 -0.003858 -0.007113 0.013065 -0.084681 0.009278 0.061606 -0.135605 -0.049061 0.016939 0.093177 0.044400 -0.003246 0.153727 0.036902 0.033454 -0.019347 -0.016304 -0.004672 0.098153 0.026940 0.091487 0.098230 -0.060728 0.079308 -0.048837 0.049123 0.109093 -0.037728 0.096776 -0.065414 0.025478 0.105321 -0.037531 -0.086120 0.071593 -0.032246 -0.014722 -0.013469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zog7jZ51LOV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c559556b-3e5b-4985-d8c2-f0526aa3689a"
      },
      "source": [
        "!wc -l model.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "163474 model.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTqeJZ961eBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeding_index={}\n",
        "\n",
        "f=open('/content/model.txt',encoding='utf-8')\n",
        "\n",
        "for i,line in enumerate(f):\n",
        "    if i==0:continue\n",
        "    values=line.split()\n",
        "    word=values[0].split('_')[0]\n",
        "    coefs=np.asarray(values[1:],dtype='float32')\n",
        "    embeding_index[word]=coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJAwJdq21mHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34fce5cc-9c3b-4847-88bd-cf6f4ce7faa7"
      },
      "source": [
        "len(embeding_index['say'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6upj6dTf2IP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text_tokeniser -> key: number, value: word (from ur vocab) -> 654\n",
        "#embedding_index -> key: word, value: vector (global) ->163474\n",
        "\n",
        "#embedding_matrix -> key: number, value: vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTioOY1P15Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix=np.zeros((vocab_len+1,300)) #655*300 <zeros>\n",
        "words_not_available=0\n",
        "for word,i in text_tokenizer.word_index.items():\n",
        "    embed_vector=embeding_index.get(word)\n",
        "    if embed_vector is not None:\n",
        "        embedding_matrix[i]=embed_vector\n",
        "    else:\n",
        "      words_not_available+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEniRwnX3uQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7edb7efa-a092-47ee-a6e9-79103698650e"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(655, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdrzD78D4WHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = Input(name='inputs',shape=[max_len]) #(20,)\n",
        "\n",
        "layer1 = Embedding(vocab_len+1,300,input_length=max_len,weights=[embedding_matrix],trainable=True,\n",
        "                  mask_zero=True)(inputs) #(20,100)\n",
        "layer2 = LSTM(64)(layer1) #(64,)\n",
        "layer3 = Dense(256,name='FC1')(layer2) #(256,)\n",
        "layer4 = Activation('relu')(layer3)\n",
        "layer5 = Dropout(0.5)(layer4)\n",
        "layer6 = Dense(8,name='out_layer')(layer5) #(8,)\n",
        "layer7 = Activation('softmax')(layer6)\n",
        "model2 = Model(inputs=inputs,outputs=layer7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2wGgpe8zVB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "e4e08c06-b30f-4f88-8b6d-ed7e652411df"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, 20)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      (None, 20, 300)           196500    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "FC1 (Dense)                  (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "out_layer (Dense)            (None, 8)                 2056      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 308,636\n",
            "Trainable params: 308,636\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HT2iJpbHivt",
        "colab_type": "text"
      },
      "source": [
        "Let's build a 3 dim array. The dimensions are samples, steps and unique words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSG06CjZMFdw",
        "colab_type": "text"
      },
      "source": [
        "### 4. Build LSTM Model\n",
        "\n",
        "Build a class lstm_model_class that has three methods(methods are similar to functions) in a class, they are:\n",
        "\n",
        "\n",
        "\n",
        "1.   Build a lstm model \n",
        "2.   Train the created lstm model on the train data\n",
        "3.   Predict the output on the train data\n",
        "\n",
        "Note: Building a Class with 3 methods helps in tying all these three functions to the same object at the same instance. \n",
        "\n",
        "Process of Building a LSTM Model:\n",
        "\n",
        "\n",
        "\n",
        "1.   Build an embedded layer with dimensions as number of steps and input dimensions. \n",
        "2.   Build an LSTM layer with number of steps equal to memory units.  \n",
        "3.   Then, build a dense layer which is fully connected layer that represents a matrix vector multiplication. \n",
        "4.   Apply the relu function after normalization and scaling of the activations. This is the standard activation function used. \n",
        "5.   Finally, build the output layer.\n",
        "\n",
        "Note: \n",
        "1. activation function for multi-class classification problem - softmax\n",
        "2. loss function is categorical cross entropy. \n",
        "3. performace metric - Area under the curve(AUC)\n",
        "4. optimizer would be the Adam optimizer. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRmkHRRUU5Uv",
        "colab_type": "text"
      },
      "source": [
        "Build the model on the necessary inputs.\n",
        "We define the number of steps as the , output shape and input dimensio appropriately. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0HQ71DqWA_p",
        "colab_type": "text"
      },
      "source": [
        "### 5. Train & Evaluate the model\n",
        "In our last step, we will train and evaluate the model and check the performance metrics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKS5QDZcy3UH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f24338b-3c15-4759-8923-2a1022996a7f"
      },
      "source": [
        "final_model.train_lstm_model(trans_matrix_train, train_target_encoded,\n",
        "           0.2, 60) #Model takes train data, train target variable, validation split(here it is 80:20) and number of epochs. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "121/121 [==============================] - 2s 13ms/step - loss: 7.2804 - auc: 0.7257 - val_loss: 5.8829 - val_auc: 0.6351\n",
            "Epoch 2/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 4.1178 - auc: 0.9361 - val_loss: 3.4945 - val_auc: 0.9867\n",
            "Epoch 3/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 2.4641 - auc: 0.9746 - val_loss: 2.5842 - val_auc: 0.9226\n",
            "Epoch 4/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 1.6020 - auc: 0.9844 - val_loss: 1.4742 - val_auc: 0.9943\n",
            "Epoch 5/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 1.1521 - auc: 0.9908 - val_loss: 0.9134 - val_auc: 0.9963\n",
            "Epoch 6/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.9257 - auc: 0.9938 - val_loss: 0.6942 - val_auc: 0.9979\n",
            "Epoch 7/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.7929 - auc: 0.9952 - val_loss: 0.6916 - val_auc: 0.9956\n",
            "Epoch 8/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.6836 - auc: 0.9967 - val_loss: 0.6047 - val_auc: 0.9946\n",
            "Epoch 9/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.6265 - auc: 0.9978 - val_loss: 0.5225 - val_auc: 0.9971\n",
            "Epoch 10/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.5694 - auc: 0.9977 - val_loss: 0.4863 - val_auc: 0.9959\n",
            "Epoch 11/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.5111 - auc: 0.9986 - val_loss: 0.4461 - val_auc: 0.9946\n",
            "Epoch 12/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.4760 - auc: 0.9986 - val_loss: 0.4722 - val_auc: 0.9933\n",
            "Epoch 13/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.4369 - auc: 0.9985 - val_loss: 0.3985 - val_auc: 0.9961\n",
            "Epoch 14/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.4131 - auc: 0.9990 - val_loss: 0.3889 - val_auc: 0.9947\n",
            "Epoch 15/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.3834 - auc: 0.9989 - val_loss: 0.3495 - val_auc: 0.9959\n",
            "Epoch 16/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.3602 - auc: 0.9989 - val_loss: 0.3221 - val_auc: 0.9954\n",
            "Epoch 17/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.3383 - auc: 0.9993 - val_loss: 0.3535 - val_auc: 0.9979\n",
            "Epoch 18/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.3251 - auc: 0.9992 - val_loss: 0.3052 - val_auc: 0.9943\n",
            "Epoch 19/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.3059 - auc: 0.9994 - val_loss: 0.3031 - val_auc: 0.9944\n",
            "Epoch 20/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2970 - auc: 0.9994 - val_loss: 0.2895 - val_auc: 0.9943\n",
            "Epoch 21/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2737 - auc: 0.9996 - val_loss: 0.2777 - val_auc: 0.9946\n",
            "Epoch 22/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2605 - auc: 0.9997 - val_loss: 0.2674 - val_auc: 0.9967\n",
            "Epoch 23/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2537 - auc: 0.9995 - val_loss: 0.2519 - val_auc: 0.9953\n",
            "Epoch 24/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2391 - auc: 0.9996 - val_loss: 0.2504 - val_auc: 0.9950\n",
            "Epoch 25/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2284 - auc: 0.9997 - val_loss: 0.2517 - val_auc: 0.9947\n",
            "Epoch 26/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2231 - auc: 0.9996 - val_loss: 0.2403 - val_auc: 0.9952\n",
            "Epoch 27/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2127 - auc: 0.9997 - val_loss: 0.2336 - val_auc: 0.9954\n",
            "Epoch 28/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2064 - auc: 0.9998 - val_loss: 0.2217 - val_auc: 0.9964\n",
            "Epoch 29/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.2003 - auc: 0.9997 - val_loss: 0.2137 - val_auc: 0.9961\n",
            "Epoch 30/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1948 - auc: 0.9997 - val_loss: 0.2153 - val_auc: 0.9962\n",
            "Epoch 31/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1889 - auc: 0.9998 - val_loss: 0.2108 - val_auc: 0.9958\n",
            "Epoch 32/60\n",
            "121/121 [==============================] - 1s 9ms/step - loss: 0.1831 - auc: 0.9997 - val_loss: 0.2085 - val_auc: 0.9958\n",
            "Epoch 33/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1763 - auc: 0.9997 - val_loss: 0.2139 - val_auc: 0.9952\n",
            "Epoch 34/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1727 - auc: 0.9998 - val_loss: 0.1969 - val_auc: 0.9966\n",
            "Epoch 35/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1666 - auc: 0.9998 - val_loss: 0.1971 - val_auc: 0.9974\n",
            "Epoch 36/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1619 - auc: 0.9997 - val_loss: 0.1970 - val_auc: 0.9971\n",
            "Epoch 37/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1584 - auc: 0.9999 - val_loss: 0.1929 - val_auc: 0.9959\n",
            "Epoch 38/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1532 - auc: 0.9998 - val_loss: 0.1869 - val_auc: 0.9962\n",
            "Epoch 39/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1491 - auc: 0.9999 - val_loss: 0.1851 - val_auc: 0.9962\n",
            "Epoch 40/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1436 - auc: 0.9999 - val_loss: 0.1790 - val_auc: 0.9971\n",
            "Epoch 41/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1397 - auc: 0.9999 - val_loss: 0.1791 - val_auc: 0.9972\n",
            "Epoch 42/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1369 - auc: 0.9998 - val_loss: 0.1696 - val_auc: 0.9968\n",
            "Epoch 43/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1323 - auc: 0.9999 - val_loss: 0.1667 - val_auc: 0.9968\n",
            "Epoch 44/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1305 - auc: 1.0000 - val_loss: 0.1665 - val_auc: 0.9963\n",
            "Epoch 45/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1288 - auc: 0.9999 - val_loss: 0.1726 - val_auc: 0.9954\n",
            "Epoch 46/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1247 - auc: 0.9999 - val_loss: 0.1623 - val_auc: 0.9958\n",
            "Epoch 47/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1204 - auc: 0.9999 - val_loss: 0.1611 - val_auc: 0.9969\n",
            "Epoch 48/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1188 - auc: 0.9999 - val_loss: 0.1555 - val_auc: 0.9963\n",
            "Epoch 49/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1150 - auc: 0.9999 - val_loss: 0.1568 - val_auc: 0.9963\n",
            "Epoch 50/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1131 - auc: 0.9999 - val_loss: 0.1850 - val_auc: 0.9977\n",
            "Epoch 51/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1148 - auc: 1.0000 - val_loss: 0.1697 - val_auc: 0.9968\n",
            "Epoch 52/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1145 - auc: 0.9999 - val_loss: 0.1500 - val_auc: 0.9960\n",
            "Epoch 53/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1036 - auc: 0.9999 - val_loss: 0.1485 - val_auc: 0.9965\n",
            "Epoch 54/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1037 - auc: 0.9999 - val_loss: 0.1422 - val_auc: 0.9970\n",
            "Epoch 55/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1016 - auc: 1.0000 - val_loss: 0.1455 - val_auc: 0.9960\n",
            "Epoch 56/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1021 - auc: 0.9999 - val_loss: 0.1454 - val_auc: 0.9964\n",
            "Epoch 57/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.0989 - auc: 1.0000 - val_loss: 0.1430 - val_auc: 0.9970\n",
            "Epoch 58/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.0972 - auc: 1.0000 - val_loss: 0.1445 - val_auc: 0.9966\n",
            "Epoch 59/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.1007 - auc: 1.0000 - val_loss: 0.1559 - val_auc: 0.9950\n",
            "Epoch 60/60\n",
            "121/121 [==============================] - 1s 8ms/step - loss: 0.0949 - auc: 0.9999 - val_loss: 0.1498 - val_auc: 0.9959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BvoHJ2z0ZoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "0f060d3e-add7-4f0b-b7c6-9e5189992211"
      },
      "source": [
        "pred_train= encode_target.inverse_transform(final_model.predict_lstm_model(trans_matrix_train)) #Predict on the train matrix and look at the performance\n",
        "print(classification_report(train.target, pred_train)) #Print the classification report"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "  atis_abbreviation       0.96      1.00      0.98       147\n",
            "      atis_aircraft       0.97      0.96      0.97        81\n",
            "       atis_airfare       0.99      0.99      0.99       423\n",
            "       atis_airline       0.98      0.96      0.97       157\n",
            "        atis_flight       1.00      1.00      1.00      3666\n",
            "   atis_flight_time       0.98      0.94      0.96        54\n",
            "atis_ground_service       1.00      1.00      1.00       255\n",
            "      atis_quantity       1.00      1.00      1.00        51\n",
            "\n",
            "           accuracy                           0.99      4834\n",
            "          macro avg       0.98      0.98      0.98      4834\n",
            "       weighted avg       0.99      0.99      0.99      4834\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nueVLp5cYAOa",
        "colab_type": "text"
      },
      "source": [
        "F1 and weighted avg are excellent. We can now move to implement this model on test data and see how it is performing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pynraJ0I0ZwG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "949a7f59-485c-4770-c62a-1dce0dcaef13"
      },
      "source": [
        "pred_test= encode_target.inverse_transform(final_model.predict_lstm_model(trans_matrix_test)) #Predict on the test data\n",
        "print(classification_report(test.target, pred_test)) #Print the classification report"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "  atis_abbreviation       0.80      1.00      0.89        33\n",
            "      atis_aircraft       0.83      0.56      0.67         9\n",
            "       atis_airfare       0.98      1.00      0.99        48\n",
            "       atis_airline       0.97      0.76      0.85        38\n",
            "        atis_flight       0.99      0.99      0.99       632\n",
            "   atis_flight_time       0.50      1.00      0.67         1\n",
            "atis_ground_service       1.00      0.94      0.97        36\n",
            "      atis_quantity       0.43      1.00      0.60         3\n",
            "\n",
            "           accuracy                           0.97       800\n",
            "          macro avg       0.81      0.91      0.83       800\n",
            "       weighted avg       0.98      0.97      0.97       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jczYwlA88qF3",
        "colab_type": "text"
      },
      "source": [
        "F1 and weighted avg are excellent. We can settle with this model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6Uc3lq1YisX",
        "colab_type": "text"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "\n",
        "*   Try changing some of the parameters and see if there could be any change in the performance metrics. \n",
        "\n"
      ]
    }
  ]
}